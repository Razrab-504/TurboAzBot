import asyncio
import aiohttp
import random
import logging
from bs4 import BeautifulSoup
import ssl
import certifi

from src.db.session import Local_Session

from src.db.crud.advertisement_crud import get_ad_by_id, create_ad
from src.db.crud.sent_ad_crud import is_ad_sent_to_user, create_sent_ad
from src.db.crud.user_crud import update_user
from src.db.models import User
from sqlalchemy import select
from sqlalchemy.orm import joinedload
from aiogram import Bot
import datetime
import os

async def parse_page(url: str, max_retries: int = 3) -> list:
    """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ Scrape.do API"""
    
    api_key = os.getenv("SCRAPING_API_KEY")
    if not api_key:
        logging.error("SCRAPING_API_KEY not set")
        return []

    for attempt in range(max_retries):
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            }

            # –ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è SSL –¥–ª—è Render
            ssl_context = ssl.create_default_context(cafile=certifi.where())
            ssl_context.check_hostname = True
            ssl_context.verify_mode = ssl.CERT_REQUIRED
            
            connector = aiohttp.TCPConnector(ssl=ssl_context, limit=10)
            timeout = aiohttp.ClientTimeout(total=60)
            
            # Scrape.do –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ GET –∑–∞–ø—Ä–æ—Å–µ
            async with aiohttp.ClientSession(headers=headers, timeout=timeout, connector=connector) as session:
                auth = aiohttp.BasicAuth(api_key, 'scraperapi')
                
                scrape_url = 'https://api.scrape.do/'
                params = {
                    'url': url,
                    'render': 'false'
                }
                
                async with session.get(scrape_url, params=params, auth=auth) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        logging.error(f"Scrape.do error {response.status}: {error_text[:200]}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(random.uniform(3, 7))
                            continue
                        else:
                            return []
                    html = await response.text()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ—à–∏–±–∫–∞ –ª–∏
            if "error" in html.lower() or len(html) < 500:
                logging.warning(f"Invalid response from Scrape.do on attempt {attempt + 1}, length: {len(html)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(random.uniform(5, 10))
                    continue
                else:
                    return []

            logging.info(f"–£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω HTML –æ—Ç Scrape.do –¥–ª—è {url}")

            # –ü–∞—Ä—Å–∏–Ω–≥ HTML
            soup = BeautifulSoup(html, 'html.parser')
            ads = []
            cards = soup.find_all('div', class_='products-i')
            logging.info(f"–ü–∞—Ä—Å–∏–Ω–≥ {url}: –Ω–∞–π–¥–µ–Ω–æ {len(cards)} –∫–∞—Ä—Ç–æ—á–µ–∫")

            for card in cards:
                link_tag = card.find('a', class_='products-i__link')
                if not link_tag:
                    continue

                ad_url = 'https://turbo.az' + link_tag['href']
                ad_id = ad_url.split('/')[-1].split('?')[0]

                title_tag = card.find('div', class_='products-i__name')
                title = title_tag.text.strip() if title_tag else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'

                price_tag = card.find('div', class_='product-price')
                price = price_tag.text.strip() if price_tag else '–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞'

                img_tag = card.find('img', class_='products-i__photo')
                img_url = img_tag.get('data-src') or img_tag.get('src', '') if img_tag else ''

                datetime_tag = card.find('div', class_='products-i__datetime')
                datetime_str = datetime_tag.text.strip() if datetime_tag else ''

                ads.append({
                    'id': ad_id,
                    'title': title,
                    'price': price,
                    'url': ad_url,
                    'img': img_url,
                    'city': datetime_str.split(',')[0] if ',' in datetime_str else '',
                    'published_at': datetime_str.split(',')[1].strip() if ',' in datetime_str else datetime_str
                })

            return ads
            
        except Exception as e:
            logging.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(random.uniform(5, 10))
                continue
            else:
                return []
    
    return []

async def notify_admins(bot: Bot, message: str):
    async with Local_Session() as session:
        result = await session.execute(select(User).where(User.role == "admin"))
        admins = result.scalars().all()
        for admin in admins:
            try:
                await bot.send_message(admin.id, f"‚ö†Ô∏è {message}")
            except Exception as e:
                logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∞–¥–º–∏–Ω—É {admin.id}: {e}")

async def check_expired_subscriptions():
    async with Local_Session() as session:
        now = datetime.datetime.utcnow()
        result = await session.execute(
            select(User)
            .where(User.subscription == True)
            .where(User.expiry_date.isnot(None))
            .where(User.expiry_date < now)
        )
        expired_users = result.scalars().all()
        for user in expired_users:
            await update_user(session, user.id, subscription=False)
            logging.info(f"–ü–æ–¥–ø–∏—Å–∫–∞ –∏—Å—Ç–µ–∫–ª–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user.id}")

async def parse_user_filters(bot: Bot):
    logging.info("–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤")
    await check_expired_subscriptions()

    async with Local_Session() as session:
        result = await session.execute(
            select(User)
            .options(joinedload(User.filters))
            .where(User.subscription == True)
        )
        users = result.scalars().unique().all()
        logging.info(f"–ù–∞–π–¥–µ–Ω–æ {len(users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –ø–æ–¥–ø–∏—Å–∫–æ–π")

        url_to_users = {}
        for user in users:
            for filter_ in user.filters:
                if filter_.query_url not in url_to_users:
                    url_to_users[filter_.query_url] = []
                url_to_users[filter_.query_url].append((user.id, filter_))

        logging.info(f"–ü–∞—Ä—Å–∏–Ω–≥ {len(url_to_users)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL")

        for url, user_filters in url_to_users.items():
            try:
                ads = await parse_page(url)
                logging.info(f"–î–ª—è URL –Ω–∞–π–¥–µ–Ω–æ {len(ads)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
                
                if len(ads) == 0:
                    logging.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–ª—è URL: {url}")
                    
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
                await notify_admins(bot, f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)[:100]}")
                continue

            for user_id, filter_ in user_filters:
                parts = filter_.label.split()
                make = parts[0].lower() if len(parts) > 0 else ''
                model = " ".join(parts[1:-1]).lower() if len(parts) > 2 else (parts[1].lower() if len(parts) > 1 else '')

                filtered_ads = []
                for ad in ads:
                    title_lower = ad['title'].lower()
                    if make and make not in title_lower:
                        continue
                    if model and model not in title_lower:
                        continue
                    filtered_ads.append(ad)

                logging.info(f"–î–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id} —Ñ–∏–ª—å—Ç—Ä '{filter_.label}' –ø—Ä–æ—à–µ–ª {len(filtered_ads)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")

                for ad in filtered_ads:
                    existing = await get_ad_by_id(session, ad['id'])
                    if not existing:
                        await create_ad(session, ad)

                    sent = await is_ad_sent_to_user(session, user_id, ad['id'])
                    if not sent:
                        city = ad.get('city', '')
                        pub = ad.get('published_at', '')
                        location = f"üìç {city}" if city else ""
                        time_info = f"‚è∞ {pub}" if pub else ""
                        caption = f"<b>{ad['title']}</b>\n\nüí∞ {ad['price']}\n{location}\n{time_info}\nüîó {ad['url']}"
                        try:
                            if ad.get('img'):
                                await bot.send_photo(
                                    chat_id=user_id,
                                    photo=ad['img'],
                                    caption=caption,
                                    parse_mode='HTML'
                                )
                            else:
                                await bot.send_message(
                                    chat_id=user_id,
                                    text=caption,
                                    parse_mode='HTML'
                                )
                            logging.info(f"–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ {ad['id']} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é {user_id}")
                        except Exception as e:
                            logging.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é {user_id}: {e}")
                            try:
                                await bot.send_message(
                                    chat_id=user_id,
                                    text=caption,
                                    parse_mode='HTML'
                                )
                            except Exception as e2:
                                logging.error(f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é {user_id}: {e2}")
                        
                        await create_sent_ad(session, user_id, ad['id'])

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É URL
            delay = random.uniform(10, 20)
            await asyncio.sleep(delay)

async def start_parsing_loop(bot: Bot):
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    while True:
        try:
            await parse_user_filters(bot)
        except Exception as e:
            logging.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            await notify_admins(bot, f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)[:100]}")
        
        delay = random.uniform(120, 180)  # 2-3 –º–∏–Ω—É—Ç—ã –º–µ–∂–¥—É —Ü–∏–∫–ª–∞–º–∏
        logging.info(f"–û–∂–∏–¥–∞–Ω–∏–µ {delay:.0f} —Å–µ–∫—É–Ω–¥ –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞")
        await asyncio.sleep(delay)